import importlib

def check_flash_attention():
    try:
        flash_attn = importlib.import_module("flash_attn")
        print("âœ… å·²å®‰è£… flash-attn")
        print("ğŸ“¦ ç‰ˆæœ¬:", flash_attn.__version__)
        return True
    except ImportError:
        print("âŒ æœªå®‰è£… flash-attnï¼Œå°†ä½¿ç”¨ PyTorch é»˜è®¤æ³¨æ„åŠ›å®ç°")
        return False

if __name__ == "__main__":
    installed = check_flash_attention()

    if installed:
        try:
            # å°è¯•å¯¼å…¥å…·ä½“å‡½æ•°
            from flash_attn import flash_attn_func
            print("ğŸ” flash_attn_func å¯ç”¨ï¼Œå¯ä»¥è¿›è¡Œé«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—")
        except ImportError:
            print("âš ï¸ flash-attn å·²å®‰è£…ï¼Œä½†æ²¡æœ‰æ‰¾åˆ° flash_attn_funcï¼Œå¯èƒ½ç‰ˆæœ¬ä¸å®Œæ•´")
